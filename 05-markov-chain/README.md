# üìÇ Proyecto 05: Generador de Texto (Markov Chains)

### üéØ ¬øPor qu√© este proyecto?
Este proyecto representa el **"Habla"** de nuestra IA. Hasta ahora, hemos clasificado y limpiado datos, pero no hemos generado contenido nuevo.

Aqu√≠ desmitificamos la **IA Generativa**. Implementamos una **Cadena de Markov**, un modelo probabil√≠stico que predice la "siguiente palabra" bas√°ndose en el estado actual. Es el antepasado l√≥gico de los **LLMs (Large Language Models)** modernos.

**El objetivo de aprendizaje:**
*   Comprender el concepto de **Next Token Prediction** (Predicci√≥n del siguiente token), que es la base de c√≥mo escribe ChatGPT.
*   Entender la **"Alucinaci√≥n"**: Veremos c√≥mo la m√°quina inventa oraciones gramaticalmente correctas pero sin sentido real, bas√°ndose pura estad√≠stica.
*   Aplicar **POO (Programaci√≥n Orientada a Objetos)**: En lugar de funciones sueltas, construiremos una Clase `MarkovChain` que tenga "memoria" (entrenamiento) y "m√©todos" (generaci√≥n).

### üõ†Ô∏è Conceptos T√©cnicos Aplicados
*   **POO (Clases y Objetos):** Encapsulamiento de la l√≥gica del modelo.
*   **Estructuras de Datos Complejas:** Uso de diccionarios donde los valores son listas (`{ "palabra": ["opcion_a", "opcion_b"] }`).
*   **Probabilidad y Aleatoriedad:** Uso del m√≥dulo `random` para seleccionar la siguiente palabra bas√°ndose en la frecuencia de aparici√≥n en el texto de entrenamiento.
*   **Ingesta de Texto:** Procesamiento de corpus de texto real (libros, art√≠culos) para "entrenar" el modelo.

### üöÄ C√≥mo ejecutarlo
1.  Navegar a la carpeta: `cd 05-markov-chain`
2.  Asegurarse de tener un archivo de texto para entrenar (ej: `libro.txt`) o usar el texto de prueba incluido.
3.  Ejecutar el script:
    ```bash
    python main.py
    ```

---
**Autor:** Brian Suhit  
*Estudiante de Tecnicatura Universitaria en Desarrollo de Aplicaciones Inform√°ticas (TUDAI).*